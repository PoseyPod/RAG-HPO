{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Modified HPO Extraction FastAPI with New Pipeline\n",
    "Three-step process: Extract -> Normalize -> Vector Retrieval\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_llm_clients import LangchainOpenAIClient, LangchainGeminiClient\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ======================= Global Variables =======================\n",
    "# llm_client = None\n",
    "# embeddings_model = None\n",
    "# faiss_index = None\n",
    "# embedded_documents = None\n",
    "# system_message_extract = \"\"\n",
    "# system_message_normalize = \"\"\n",
    "# FLAG_FILE = \"gemini.flag\"\n",
    "\n",
    "# # --- [修改 1/4]: 新增全域變數來儲存設定 ---\n",
    "# app_config: Dict[str, Any] = {}\n",
    "# embedding_model_name = 'pritamdeka/SapBERT-mnli-snli-scinli-scitail-mednli-stsb'\n",
    "# #embedding_model_name = 'FremyCompany/BioLORD-2023'\n",
    "# #meta_path = 'deeprare_hpo_meta.json'\n",
    "# meta_path = 'hpo_meta.json'\n",
    "# #vec_path= 'deeprare_hpo_embedded.npz'\n",
    "# vec_path= 'hpo_embedded.npz'\n",
    "\n",
    "# system_prompt_path = \"gemini_system_prompts.json\"\n",
    "\n",
    "# ====================== LLM Output Schema ======================#\n",
    "class HPOPhenotype(BaseModel):\n",
    "    HPO: str = Field(description=\"HPO ID in format HP:0000000\")\n",
    "    Phenotype: str = Field(description=\"Clinical phenotype description\")\n",
    "\n",
    "class HPOExtractionResult(BaseModel):\n",
    "    \"\"\"A container for a list of extracted HPO phenotypes.\"\"\"\n",
    "    phenotypes: List[HPOPhenotype] = Field(description=\"A list of HPO phenotypes extracted from the clinical text.\")\n",
    "\n",
    "class PhenotypeNormalization(BaseModel):\n",
    "    original_term: str = Field(description=\"Original phenotype description\")\n",
    "    hpo_term: str = Field(description=\"Standardized HPO term in English, or 'none' if not found\")\n",
    "\n",
    "# ======================= Simple Cache =======================\n",
    "# ... (此區塊程式碼不變)\n",
    "class SimpleCache:\n",
    "    \"\"\"Lightweight caching with file persistence\"\"\"\n",
    "    def __init__(self, cache_file: str = \"llm_cache.pkl\"):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self._load_cache()\n",
    "\n",
    "    def _load_cache(self) -> dict:\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except: return {}\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache(self):\n",
    "        try:\n",
    "            with open(f\"{self.cache_file}.tmp\", 'wb') as f:\n",
    "                pickle.dump(self.cache, f)\n",
    "            os.replace(f\"{self.cache_file}.tmp\", self.cache_file)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to save cache: {e}\")\n",
    "\n",
    "    def get(self, text: str, pipeline_key: str) -> Optional[Any]:\n",
    "        key = hashlib.md5(f\"{text}||{pipeline_key}\".encode()).hexdigest()\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, text: str, pipeline_key: str, response: Any):\n",
    "        key = hashlib.md5(f\"{text}||{pipeline_key}\".encode()).hexdigest()\n",
    "        self.cache[key] = response\n",
    "        if len(self.cache) % 10 == 0:\n",
    "            self._save_cache()\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.cache)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.cache.clear()\n",
    "        if os.path.exists(self.cache_file):\n",
    "            os.remove(self.cache_file)\n",
    "\n",
    "cache = SimpleCache()\n",
    "\n",
    "# ======================= Utility Functions =======================\n",
    "# ... (此區塊程式碼不變)\n",
    "def clean_note(text: str) -> str:\n",
    "    text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def _safe_json_loads(text: str) -> Optional[Dict]:\n",
    "    if not text: return None\n",
    "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
    "    markdown_json_match = re.search(r'```json\\s*\\n(.*?)\\n```', text, re.DOTALL)\n",
    "    if markdown_json_match:\n",
    "        json_content = markdown_json_match.group(1).strip()\n",
    "        try: return json.loads(json_content)\n",
    "        except json.JSONDecodeError: pass\n",
    "    markdown_match = re.search(r'```\\s*\\n(.*?)\\n```', text, re.DOTALL)\n",
    "    if markdown_match:\n",
    "        json_content = markdown_match.group(1).strip()\n",
    "        try: return json.loads(json_content)\n",
    "        except json.JSONDecodeError: pass\n",
    "    try: return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        array_match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "        if array_match:\n",
    "            try: return json.loads(array_match.group(0))\n",
    "            except json.JSONDecodeError: pass\n",
    "        object_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if object_match:\n",
    "            try: return json.loads(object_match.group(0))\n",
    "            except json.JSONDecodeError: pass\n",
    "    return None\n",
    "\n",
    "def load_vector_db(meta_path: str = 'hpo_meta.json', vec_path: str = 'hpo_embedded.npz'):\n",
    "    if not os.path.exists(meta_path) or not os.path.exists(vec_path):\n",
    "        raise FileNotFoundError(f\"DB files not found: {meta_path}, {vec_path}\")\n",
    "    with open(meta_path, 'r', encoding='utf-8') as f:\n",
    "        combined = json.load(f)\n",
    "        entries = combined.get('entries', [])\n",
    "    arr = np.load(vec_path)\n",
    "    emb_matrix = arr['emb'].astype(np.float32)\n",
    "    docs = []\n",
    "    for entry, vec in zip(entries, emb_matrix):\n",
    "        docs.append({'hp_id': entry.get('hp_id'), 'info': entry.get('info'), 'embedding': vec})\n",
    "    return docs, emb_matrix\n",
    "\n",
    "def create_faiss_index(emb_matrix: np.ndarray):\n",
    "    dim = emb_matrix.shape[1]\n",
    "    faiss.normalize_L2(emb_matrix)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(emb_matrix)\n",
    "    return index\n",
    "\n",
    "PAT_CLEAN = re.compile(r'\\s*\\([^)]*\\)\\s*')\n",
    "def clean_query_text(txt: str) -> str:\n",
    "    txt = PAT_CLEAN.sub(' ', txt)\n",
    "    txt = re.sub(r'\\s+', ' ', txt).strip().lower()\n",
    "    txt = re.sub(r'[^\\w\\s]+$', '', txt)\n",
    "    return txt\n",
    "\n",
    "def embed_query(text: str, model):\n",
    "    \"\"\"Embed query text after cleaning.\"\"\"\n",
    "    # 在嵌入前，先進行清理\n",
    "    cleaned_text = clean_query_text(text)\n",
    "    \n",
    "    vec = model.encode(cleaned_text, convert_to_numpy=True)\n",
    "    if vec.ndim == 1:\n",
    "        vec = vec.reshape(1, -1)\n",
    "    faiss.normalize_L2(vec)\n",
    "    return vec\n",
    "\n",
    "# ======================= Updated HPO Extraction Function =======================\n",
    "# ... (此區塊程式碼不變)\n",
    "def extract_hpo_terms_structured(clinical_note: str) -> Dict[str, Any]:\n",
    "    global llm_client, embeddings_model, faiss_index, embedded_documents\n",
    "    global system_message_extract, system_message_normalize, model_type\n",
    "    # PIPELINE_KEY = \"hpo_pipeline_v2.1_structured_output\"\n",
    "    # clinical_note = clean_note(clinical_note)\n",
    "    # cached_response = cache.get(clinical_note, PIPELINE_KEY)\n",
    "    # if cached_response:\n",
    "    #     logger.info(\"Using cached final result for the structured pipeline.\")\n",
    "    #     return cached_response\n",
    "    final_result = {'hpo_terms': [], 'thinking_process': []}\n",
    "    logger.info(\"Step 1: Extracting phenotypes using structured output\")\n",
    "    extracted_phenotypes = []\n",
    "    try:\n",
    "        if model_type == 'gemini':\n",
    "            raw_response_extract = llm_client.query(clinical_note, system_message_extract, response_schema=HPOExtractionResult)\n",
    "        else:\n",
    "            raw_response_extract = llm_client.query(clinical_note, system_message_extract)\n",
    "        final_result['thinking_process'].append(f\"--- Step 1: Structured Extraction ---\\nRaw response length: {len(str(raw_response_extract))} chars\")\n",
    "        parsed_extract = _safe_json_loads(raw_response_extract)\n",
    "        if isinstance(parsed_extract, dict) and 'phenotypes' in parsed_extract:\n",
    "            extracted_phenotypes = parsed_extract['phenotypes']\n",
    "        elif isinstance(parsed_extract, list):\n",
    "            for item in parsed_extract:\n",
    "                if isinstance(item, dict) and 'HPO' in item and 'Phenotype' in item:\n",
    "                    extracted_phenotypes.append(item)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Structured extraction failed: {e}\", exc_info=True)\n",
    "        final_result['thinking_process'].append(f\"Extraction error: {e}\")\n",
    "        final_result['thinking_process'] = \"\\n\\n\".join(final_result['thinking_process'])\n",
    "        return final_result\n",
    "    if not extracted_phenotypes:\n",
    "        logger.warning(\"No phenotypes extracted in structured Step 1.\")\n",
    "        final_result['thinking_process'].append(\"No phenotypes extracted.\")\n",
    "        final_result['thinking_process'] = \"\\n\\n\".join(final_result['thinking_process'])\n",
    "        return final_result\n",
    "    logger.info(f\"Structured extraction found {len(extracted_phenotypes)} phenotypes\")\n",
    "    final_result['thinking_process'].append(f\"Extracted {len(extracted_phenotypes)} phenotypes successfully\")\n",
    "    logger.info(\"Step 2: Normalizing phenotypes using structured output\")\n",
    "    normalized_phenotypes = []\n",
    "    for i, phenotype in enumerate(extracted_phenotypes):\n",
    "        phenotype_desc = phenotype.get('Phenotype', '') if isinstance(phenotype, dict) else str(phenotype)\n",
    "        if not phenotype_desc.strip(): continue\n",
    "        try:\n",
    "            if model_type == 'gemini':\n",
    "                raw_normalized_response = llm_client.query(user_input=phenotype_desc, system_message=system_message_normalize, response_schema=PhenotypeNormalization)\n",
    "            # \n",
    "            # if hasattr(llm_client, 'query_structured'):\n",
    "            #     raw_normalized_response = llm_client.query_structured(user_input=phenotype_desc, system_message=system_message_normalize, response_schema=PhenotypeNormalization)\n",
    "            else:\n",
    "                raw_normalized_response = llm_client.query(phenotype_desc, system_message_normalize)\n",
    "            normalized_response = _safe_json_loads(raw_normalized_response)\n",
    "            if (isinstance(normalized_response, dict) and normalized_response.get('hpo_term') and normalized_response.get('hpo_term') != 'none'):\n",
    "                normalized_phenotypes.append({'original_term': normalized_response.get('original_term', phenotype_desc), 'hpo_term': normalized_response.get('hpo_term')})\n",
    "                final_result['thinking_process'].append(f\"--- Step 2 Normalize '{phenotype_desc}' ---\\nNormalized to: {normalized_response.get('hpo_term')}\")\n",
    "            else:\n",
    "                final_result['thinking_process'].append(f\"--- Step 2 Normalize '{phenotype_desc}' ---\\nCould not normalize (result: {normalized_response.get('hpo_term', 'none') if isinstance(normalized_response, dict) else 'none'})\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Normalization failed for '{phenotype_desc}': {e}\")\n",
    "            final_result['thinking_process'].append(f\"--- Step 2 Normalize '{phenotype_desc}' ---\\nNormalization error: {e}\")\n",
    "            continue\n",
    "    if not normalized_phenotypes:\n",
    "        logger.warning(\"No phenotypes normalized in structured Step 2.\")\n",
    "        final_result['thinking_process'].append(\"No phenotypes successfully normalized.\")\n",
    "        final_result['thinking_process'] = \"\\n\\n\".join(final_result['thinking_process'])\n",
    "        return final_result\n",
    "    logger.info(f\"Structured normalization completed: {len(normalized_phenotypes)} phenotypes\")\n",
    "    logger.info(\"Step 3: Vector retrieval for normalized phenotypes\")\n",
    "    final_hpo_terms = []\n",
    "    for normalized in normalized_phenotypes:\n",
    "        hpo_term = normalized['hpo_term']\n",
    "        original_term = normalized['original_term']\n",
    "        try:\n",
    "            query_vec = embed_query(hpo_term, embeddings_model)\n",
    "            distances, indices = faiss_index.search(query_vec, 1)\n",
    "            if indices.size > 0 and indices[0][0] >= 0:\n",
    "                best_idx = indices[0][0]\n",
    "                best_score = distances[0][0]\n",
    "                if best_score > 0.8:\n",
    "                    best_match = embedded_documents[best_idx]\n",
    "                    final_hpo_terms.append({'phrase': original_term, 'normalized_term': hpo_term, 'hpo_id': best_match.get('hp_id'), 'hpo_description': best_match.get('info'), 'similarity_score': float(best_score)})\n",
    "                    final_result['thinking_process'].append(f\"--- Step 3 Retrieval for '{hpo_term}' ---\\n✅ Match: {best_match.get('hp_id')} - {best_match.get('info')} (score: {best_score:.3f})\")\n",
    "                else:\n",
    "                    logger.info(f\"Similarity score {best_score:.3f} too low for '{hpo_term}' (threshold: 0.8)\")\n",
    "                    final_result['thinking_process'].append(f\"--- Step 3 Retrieval for '{hpo_term}' ---\\n❌ Score {best_score:.3f} below threshold 0.8\")\n",
    "            else:\n",
    "                final_result['thinking_process'].append(f\"--- Step 3 Retrieval for '{hpo_term}' ---\\n❌ No vector match found\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Vector retrieval failed for '{hpo_term}': {e}\")\n",
    "            final_result['thinking_process'].append(f\"--- Step 3 Retrieval for '{hpo_term}' ---\\n❌ Retrieval error: {e}\")\n",
    "    final_result['hpo_terms'] = final_hpo_terms\n",
    "    final_result['thinking_process'] = \"\\n\\n\".join(final_result['thinking_process'])\n",
    "    # cache.set(clinical_note, PIPELINE_KEY, final_result)\n",
    "    # logger.info(f\"Cached structured result with {len(final_hpo_terms)} HPO terms\")\n",
    "    return final_result\n",
    "\n",
    "def extract_thinking_from_content(content: str) -> tuple:\n",
    "    if not content: return \"\", \"\"\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "        if \"choices\" in data and data[\"choices\"]:\n",
    "            message_content = data[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            if message_content: content = message_content\n",
    "    except (json.JSONDecodeError, TypeError): pass\n",
    "    think_pattern = r'<think>(.*?)</think>'\n",
    "    matches = re.findall(think_pattern, content, re.DOTALL)\n",
    "    if matches:\n",
    "        thinking_process = matches[0].strip()\n",
    "        cleaned_content = re.sub(think_pattern, '', content, flags=re.DOTALL).strip()\n",
    "        return thinking_process, cleaned_content\n",
    "    else:\n",
    "        return \"\", content\n",
    "\n",
    "# --- [修改 2/4]: 修改此函數，使其接收 config 字典 ---\n",
    "def check_and_initialize_llm(config: Dict[str, Any]):\n",
    "    \"\"\"Initialize LLM client from a config dictionary\"\"\"\n",
    "    global model_type\n",
    "\n",
    "    model_type = config.get(\"model_type\")\n",
    "    \n",
    "    # 從傳入的 config 字典中準備參數\n",
    "    common_args = {\n",
    "        \"api_key\": config.get(\"api_key\"),\n",
    "        \"base_url\": config.get(\"base_url\"),\n",
    "        \"model_name\": config.get(\"model_name\"),\n",
    "        \"temperature\": config.get(\"temperature\", 0.2),\n",
    "        \"max_tokens_per_day\": config.get(\"max_tokens_per_day\", -1),\n",
    "        \"max_queries_per_minute\": config.get(\"max_queries_per_minute\", 60),\n",
    "        \"max_tokens_per_minute\": config.get(\"max_tokens_per_minute\", 4000000),\n",
    "        \"think\": config.get(\"think\", False)\n",
    "    }\n",
    "    \n",
    "    if model_type == \"openai\":\n",
    "        del common_args['think']\n",
    "        return LangchainOpenAIClient(**common_args)\n",
    "    elif model_type == \"gemini\":\n",
    "        return LangchainGeminiClient(**common_args)\n",
    "    elif model_type == \"ollama\":\n",
    "        return OllamaClient(**common_args)\n",
    "    elif model_type == \"vllm\":\n",
    "        return vLLMClient(**common_args)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_type: {model_type}\")\n",
    "\n",
    "# ======================= Startup/Shutdown =======================\n",
    "\n",
    "async def lifespan(app: FastAPI):\n",
    "    global llm_client, embeddings_model, faiss_index, embedded_documents\n",
    "    global system_message_extract, system_message_normalize\n",
    "    global app_config, embedding_model_name  # 宣告要修改全域變數\n",
    "    global meta_path, vec_path\n",
    "    global system_prompt_path\n",
    "    \n",
    "    logger.info(\"Starting HPO Extraction API...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. 載入 system prompts\n",
    "        with open(system_prompt_path, \"r\") as f:\n",
    "            prompts = json.load(f)\n",
    "        system_message_extract = prompts.get(\"system_message_extract\", \"\")\n",
    "        system_message_normalize = prompts.get(\"system_message_normalize\", \"\")\n",
    "        \n",
    "        if not all([system_message_extract, system_message_normalize]):\n",
    "            raise ValueError(\"system_message_extract or system_message_normalize are missing from system_prompts.json\")\n",
    "\n",
    "        # 2. 載入 .flag 設定檔並存到全域變數\n",
    "        if not os.path.exists(FLAG_FILE):\n",
    "            raise FileNotFoundError(f\"Flag file '{FLAG_FILE}' not found.\")\n",
    "        with open(FLAG_FILE, \"r\") as f:\n",
    "            app_config = json.load(f)\n",
    "\n",
    "        # 3. 初始化 LLM Client (傳入已載入的 config)\n",
    "        llm_client = check_and_initialize_llm(app_config)\n",
    "        \n",
    "        # 4. 初始化 Embedding Model 並儲存名稱\n",
    "        embeddings_model = SentenceTransformer(embedding_model_name)\n",
    "        \n",
    "        # 5. 載入向量資料庫\n",
    "        docs, emb_matrix = load_vector_db(meta_path, vec_path)\n",
    "        embedded_documents = docs\n",
    "        faiss_index = create_faiss_index(emb_matrix)\n",
    "        \n",
    "        logger.info(f\"API initialized with model type: '{app_config.get('model_type')}' and cache size: {cache.size()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize: {e}\")\n",
    "        raise\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    cache._save_cache()\n",
    "    logger.info(\"Shutting down...\")\n",
    "\n",
    "# ======================= Pydantic Models =======================\n",
    "# ... (此區塊程式碼不變)\n",
    "class AnnotateRequest(BaseModel):\n",
    "    text: str = Field(..., min_length=1)\n",
    "class AnnotateResponse(BaseModel):\n",
    "    text: str\n",
    "    hpo_terms: str\n",
    "    hpo_ids: str\n",
    "    processing_time: float\n",
    "    cached: bool = False\n",
    "    thinking_process: str = Field(default=\"\", description=\"AI reasoning process from the multi-step pipeline\")\n",
    "class BatchRequest(BaseModel):\n",
    "    texts: List[str] = Field(..., min_items=1)\n",
    "class BatchResponse(BaseModel):\n",
    "    results: List[AnnotateResponse]\n",
    "    total_time: float\n",
    "    cached_count: int\n",
    "    count: int\n",
    "    total_processing_time: float\n",
    "\n",
    "\n",
    "async def root():\n",
    "    return {\n",
    "        \"service\": \"Modified HPO Extraction API\",\n",
    "        \"version\": \"2.0\",\n",
    "        \"cache_size\": cache.size(),\n",
    "        \"pipeline\": \"Extract -> Normalize -> Retrieve\"\n",
    "    }\n",
    "\n",
    "async def get_configuration():\n",
    "    \"\"\"\n",
    "    Display the current application configuration, excluding sensitive information.\n",
    "    \"\"\"\n",
    "    # 複製一份設定，避免修改到原始的 global config\n",
    "    safe_config = app_config.copy()\n",
    "    \n",
    "    # 移除 api_key，確保不會洩漏\n",
    "    safe_config.pop(\"api_key\", None)\n",
    "    \n",
    "    # 加入 embedding model 資訊\n",
    "    safe_config[\"embedding_model\"] = embedding_model_name\n",
    "    \n",
    "    return safe_config\n",
    "\n",
    "async def health():\n",
    "# ... (之後的所有端點和主程式碼都不變)\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"cache_size\": cache.size(),\n",
    "        \"timestamp\": time.time()\n",
    "    }\n",
    "\n",
    "def annotate(clinical_text):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        was_cached = bool(cache.get(clean_note(request.text), \"hpo_pipeline_v2.1_structured_output\"))\n",
    "        hpo_result = extract_hpo_terms_structured(request.text)\n",
    "        hpo_terms = hpo_result.get('hpo_terms', [])\n",
    "        thinking_process = hpo_result.get('thinking_process', '')\n",
    "        term_parts = []\n",
    "        id_parts = []\n",
    "        seen_ids = set()\n",
    "        for term in hpo_terms:\n",
    "            phrase = term.get('phrase', '')\n",
    "            hpo_id = term.get('hpo_id', '')\n",
    "            if hpo_id and hpo_id.startswith(\"HP:\") and hpo_id not in seen_ids:\n",
    "                term_parts.append(f\"{phrase} ({hpo_id})\")\n",
    "                id_parts.append(hpo_id)\n",
    "                seen_ids.add(hpo_id)\n",
    "        return AnnotateResponse(text=request.text, hpo_terms=\";\".join(term_parts), hpo_ids=\";\".join(id_parts), processing_time=time.time() - start_time, cached=was_cached, thinking_process=thinking_process)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Annotation error: {e}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_demo_text='A syndrome of brachydactyly (absence of some middle or distal phalanges), aplastic or hypoplastic nails, symphalangism (ankylois of proximal interphalangeal joints), synostosis of some carpal and tarsal bones, craniosynostosis, and dysplastic hip joints is reported in five members of an Italian family. It may represent a previously undescribed autosomal dominant trait.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: pritamdeka/SapBERT-mnli-snli-scinli-scitail-mednli-stsb\n",
      "INFO:__main__:API initialized with model type: 'openai' and cache size: 1\n"
     ]
    }
   ],
   "source": [
    "llm_client = None\n",
    "embeddings_model = None\n",
    "faiss_index = None\n",
    "embedded_documents = None\n",
    "system_message_extract = \"\"\n",
    "system_message_normalize = \"\"\n",
    "\n",
    "FLAG_FILE = \"openai.flag\"\n",
    "system_prompt_path = \"openai_system_prompts2.json\"\n",
    "\n",
    "# FLAG_FILE = \"gemini.flag\"\n",
    "# system_prompt_path = \"gemini_system_prompts.json\"\n",
    "\n",
    "\n",
    "\n",
    "# --- [修改 1/4]: 新增全域變數來儲存設定 ---\n",
    "app_config: Dict[str, Any] = {}\n",
    "embedding_model_name = 'pritamdeka/SapBERT-mnli-snli-scinli-scitail-mednli-stsb'\n",
    "#embedding_model_name = 'FremyCompany/BioLORD-2023'\n",
    "#meta_path = 'deeprare_hpo_meta.json'\n",
    "meta_path = 'hpo_meta.json'\n",
    "#vec_path= 'deeprare_hpo_embedded.npz'\n",
    "vec_path= 'hpo_embedded.npz'\n",
    "\n",
    "\n",
    "# 1. 載入 system prompts\n",
    "with open(system_prompt_path, \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "system_message_extract = prompts.get(\"system_message_extract\", \"\")\n",
    "system_message_normalize = prompts.get(\"system_message_normalize\", \"\")\n",
    "\n",
    "if not all([system_message_extract, system_message_normalize]):\n",
    "    raise ValueError(\"system_message_extract or system_message_normalize are missing from system_prompts.json\")\n",
    "\n",
    "# 2. 載入 .flag 設定檔並存到全域變數\n",
    "if not os.path.exists(FLAG_FILE):\n",
    "    raise FileNotFoundError(f\"Flag file '{FLAG_FILE}' not found.\")\n",
    "with open(FLAG_FILE, \"r\") as f:\n",
    "    app_config = json.load(f)\n",
    "\n",
    "# 3. 初始化 LLM Client (傳入已載入的 config)\n",
    "llm_client = check_and_initialize_llm(app_config)\n",
    "\n",
    "# 4. 初始化 Embedding Model 並儲存名稱\n",
    "embeddings_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "# 5. 載入向量資料庫\n",
    "docs, emb_matrix = load_vector_db(meta_path, vec_path)\n",
    "embedded_documents = docs\n",
    "faiss_index = create_faiss_index(emb_matrix)\n",
    "\n",
    "logger.info(f\"API initialized with model type: '{app_config.get('model_type')}' and cache size: {cache.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_demo = extract_hpo_terms_structured(input_demo_text)\n",
    "\n",
    "def annotate_demo(clinical_summary):\n",
    "    hpo_result = extract_hpo_terms_structured(clinical_summary)\n",
    "    hpo_terms = hpo_result.get('hpo_terms', [])\n",
    "    thinking_process = hpo_result.get('thinking_process', '')\n",
    "    term_parts = []\n",
    "    id_parts = []\n",
    "    seen_ids = set()\n",
    "    for term in hpo_terms:\n",
    "        phrase = term.get('phrase', '')\n",
    "        hpo_id = term.get('hpo_id', '')\n",
    "        if hpo_id and hpo_id.startswith(\"HP:\") and hpo_id not in seen_ids:\n",
    "            term_parts.append(f\"{phrase} ({hpo_id})\")\n",
    "            id_parts.append(hpo_id)\n",
    "            seen_ids.add(hpo_id)\n",
    "    return {'text':clinical_summary,\n",
    "            'hop_term':\";\".join(term_parts),\n",
    "            'hpo_ids':\";\".join(id_parts),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Step 1: Extracting phenotypes using structured output\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Structured extraction found 7 phenotypes\n",
      "INFO:__main__:Step 2: Normalizing phenotypes using structured output\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Structured normalization completed: 7 phenotypes\n",
      "INFO:__main__:Step 3: Vector retrieval for normalized phenotypes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c8666957f24355b06e20f8c00ac788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64fa050727147a0a1cc85689fc27aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb36471b3a3240d1b4dce93f3e2cbb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042eb9a9431f4f10a46af395a654c36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70215e7e504c4e42900f6c71e785bc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79133973462a4fcbae2b2d472c764d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebf0c12dbfe4bc1a6186048df4b3c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "hop_term\n",
      "hpo_ids\n"
     ]
    }
   ],
   "source": [
    "output_demo = annotate_demo(input_demo_text)\n",
    "for key, val in output_demo.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'A syndrome of brachydactyly (absence of some middle or distal phalanges), aplastic or hypoplastic nails, symphalangism (ankylois of proximal interphalangeal joints), synostosis of some carpal and tarsal bones, craniosynostosis, and dysplastic hip joints is reported in five members of an Italian family. It may represent a previously undescribed autosomal dominant trait.',\n",
       " 'hop_term': 'Brachydactyly (HP:0001156);Aplastic nails (HP:0001792);Symphalangism (HP:0001204);Synostosis of carpal bones (HP:0009702);Synostosis of tarsal bones (HP:0008368);Craniosynostosis (HP:0001363);Dysplastic hip joints (HP:0001385)',\n",
       " 'hpo_ids': 'HP:0001156;HP:0001792;HP:0001204;HP:0009702;HP:0008368;HP:0001363;HP:0001385'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-hpo-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
